Qwen é«˜æ•ˆå¾®è°ƒé¡¹ç›®
ä½¿ç”¨ transformers å’Œ peft åº“å¯¹ Qwen å¤§æ¨¡å‹è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆLoRA/Prompt Tuningï¼‰ï¼Œæ”¯æŒåŠç²¾åº¦ï¼ˆFP16ï¼‰è®­ç»ƒä»¥é™ä½æ˜¾å­˜å ç”¨ã€‚

ğŸ“Œ åŠŸèƒ½ç‰¹æ€§
âœ… å‚æ•°é«˜æ•ˆå¾®è°ƒï¼šæ”¯æŒ LoRA å’Œ Prompt Tuningï¼Œä»…ä¼˜åŒ–å°‘é‡å‚æ•°ã€‚

ğŸš€ åŠç²¾åº¦è®­ç»ƒï¼šFP16 æ··åˆç²¾åº¦åŠ é€Ÿï¼Œæ˜¾å­˜å ç”¨å‡å°‘ 50%ã€‚

ğŸ¤– å¤šä»»åŠ¡é€‚é…ï¼šæ”¯æŒæ–‡æœ¬ç”Ÿæˆã€åˆ†ç±»ç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚

ğŸ’¾ è½»é‡å­˜å‚¨ï¼šä»…ä¿å­˜é€‚é…å™¨æƒé‡ï¼ˆLoRA/Promptï¼‰ï¼Œæ— éœ€å…¨æ¨¡å‹å‚æ•°ã€‚

ğŸ› ï¸ ç¯å¢ƒå®‰è£…
bash
pip install torch transformers peft accelerate datasets bitsandbytes
æ¨è Python 3.8+ å’Œ CUDA 11.7+ã€‚

éœ€ NVIDIA GPUï¼ˆæ”¯æŒ FP16 è®¡ç®—ï¼‰ã€‚

ğŸš€ å¿«é€Ÿå¼€å§‹
1. æ•°æ®å‡†å¤‡
æ ¼å¼ï¼šJSON æ–‡ä»¶ï¼ŒåŒ…å« instruction/input/output å­—æ®µï¼ˆç¤ºä¾‹è§ data/ ç›®å½•ï¼‰ã€‚

ç¤ºä¾‹æ•°æ®ï¼š

json
[{"instruction": "ç¿»è¯‘ä¸ºè‹±æ–‡", "input": "ä½ å¥½", "output": "Hello"}]
2. LoRA å¾®è°ƒ
bash
python train_lora.py \
    --model_path path/to/qwen \
    --data_path data/train.json \
    --output_dir outputs/lora \
    --fp16 True  # å¯ç”¨åŠç²¾åº¦
å…³é”®å‚æ•°ï¼š

--lora_r: LoRA ç§©ï¼ˆé»˜è®¤ 8ï¼‰

--lora_alpha: ç¼©æ”¾ç³»æ•°ï¼ˆé»˜è®¤ 32ï¼‰

--target_modules: ç›®æ ‡æ¨¡å—ï¼ˆé»˜è®¤ q_proj,k_proj,v_projï¼‰

3. Prompt Tuning å¾®è°ƒ
bash
python train_prompt_tuning.py \
    --model_path path/to/qwen \
    --data_path data/train.json \
    --num_virtual_tokens 20 \
    --prompt_init_text "åˆ†ç±»ä»»åŠ¡ï¼š" 
ğŸ“‚ ä»£ç ç»“æ„
text
.
â”œâ”€â”€ train_lora.py            # LoRA å¾®è°ƒè„šæœ¬
â”œâ”€â”€ train_prompt_tuning.py   # Prompt Tuning è„šæœ¬
â”œâ”€â”€ inference.py             # åŠ è½½é€‚é…å™¨æ¨ç†
â”œâ”€â”€ data/                    # ç¤ºä¾‹æ•°æ®
â”‚   â”œâ”€â”€ train.json
â”‚   â””â”€â”€ test.json
â””â”€â”€ outputs/                 # ä¿å­˜é€‚é…å™¨æƒé‡
âš™ï¸ å‚æ•°é…ç½®
é€šç”¨è®­ç»ƒå‚æ•°ï¼ˆTrainingArgumentsï¼‰
å‚æ•°å	è¯´æ˜
fp16	å¯ç”¨åŠç²¾åº¦è®­ç»ƒï¼ˆé»˜è®¤ Trueï¼‰
per_device_train_batch_size	æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰
learning_rate	å­¦ä¹ ç‡ï¼ˆLoRA å»ºè®® 3e-4ï¼ŒPrompt Tuning å»ºè®® 3e-2ï¼‰
LoRA ä¸“ç”¨å‚æ•°
python
LoraConfig(
    r=8,                         # ç§©
    lora_alpha=32,               # ç¼©æ”¾ç³»æ•°
    target_modules=["q_proj"],   # ç›®æ ‡æ¨¡å—
    lora_dropout=0.05
)
Prompt Tuning ä¸“ç”¨å‚æ•°
python
PromptTuningConfig(
    num_virtual_tokens=20,       # è½¯æç¤ºé•¿åº¦
    prompt_tuning_init="TEXT"    # åˆå§‹åŒ–æ–¹å¼
)
ğŸ§  æ¨ç†ç¤ºä¾‹
åŠ è½½å¾®è°ƒåçš„é€‚é…å™¨è¿›è¡Œé¢„æµ‹ï¼š

python
from peft import PeftModel

model = AutoModelForCausalLM.from_pretrained("path/to/qwen")
model = PeftModel.from_pretrained(model, "outputs/lora")  # åŠ è½½ LoRA
model.half()  # åˆ‡æ¢åˆ° FP16

inputs = tokenizer("Instruction: ç¿»è¯‘ä¸ºè‹±æ–‡\nInput: æ—©å®‰", return_tensors="pt")
outputs = model.generate(**inputs)
print(tokenizer.decode(outputs[0]))
ğŸ“Š æ€§èƒ½å¯¹æ¯”
æ–¹æ³•	å¯è®­ç»ƒå‚æ•°é‡	æ˜¾å­˜å ç”¨ï¼ˆQwen-7Bï¼‰	è®­ç»ƒé€Ÿåº¦
å…¨å‚æ•°å¾®è°ƒ	7B	80GB+	1x
LoRA (FP16)	0.1%	~12GB	1.5x
Prompt Tuning	0.01%	~8GB	2x
â“ å¸¸è§é—®é¢˜
æ˜¾å­˜ä¸è¶³ï¼š

å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼šgradient_checkpointing=True

ä½¿ç”¨ 4-bit é‡åŒ–ï¼šåœ¨ from_pretrained ä¸­æ·»åŠ  BitsAndBytesConfigã€‚

NaN æŸå¤±ï¼š

é™ä½å­¦ä¹ ç‡æˆ–å¯ç”¨æ¢¯åº¦ç¼©æ”¾ï¼šfp16_full_eval=Trueã€‚

å¦‚ä½•é€‚é…å…¶ä»–ä»»åŠ¡ï¼Ÿ

ä¿®æ”¹ data.json ä¸­çš„å­—æ®µå’Œ preprocess_functionã€‚

